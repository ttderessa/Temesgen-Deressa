{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5q0sdgpsrSgozOdktr/my",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttderessa/Temesgen-Deressa/blob/main/Problematic_internet_Use_Child_Mind_Institute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \"\"\"\n",
        " Description: This script preprocesses data, trains multiple machine learning models,\n",
        " and evaluates them using a Voting Classifier for Child Mind Institute â€” Problematic Internet Use\n",
        " \"\"\"\n",
        "# ============================================\n",
        "# ============================================\n",
        "# 1. Data Manipulation and Numerical Operations\n",
        "# ============================================\n",
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np  # For numerical operations\n",
        "\n",
        "# ============================================\n",
        "# 2. Machine Learning Libraries\n",
        "# ============================================\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.linear_model import LogisticRegression  # For logistic regression\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,  # Random Forest Classifier\n",
        "    AdaBoostClassifier,  # Adaptive Boosting\n",
        "    GradientBoostingClassifier,  # Gradient Boosting\n",
        "    VotingClassifier  # Ensemble voting classifier\n",
        ")\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision tree\n",
        "from xgboost import XGBClassifier  # Extreme Gradient Boosting Classifier\n",
        "\n",
        "# ============================================\n",
        "# 3. Preprocessing and Feature Scaling\n",
        "# ============================================\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder  # For scaling and encoding\n",
        "from sklearn.impute import SimpleImputer  # For handling missing values (simple strategy)\n",
        "\n",
        "# Explicitly enable IterativeImputer (experimental)\n",
        "from sklearn.experimental import enable_iterative_imputer  # This is required to use IterativeImputer\n",
        "from sklearn.impute import IterativeImputer  # Advanced imputation (experimental)\n",
        "\n",
        "# ============================================\n",
        "# 4. Model Evaluation Metrics\n",
        "# ============================================\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,  # For model accuracy\n",
        "    precision_score,  # For precision metric\n",
        "    recall_score,  # For recall metric\n",
        "    f1_score,  # For F1-score\n",
        "    roc_auc_score,  # For ROC-AUC score\n",
        "    confusion_matrix  # For confusion matrix\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 5. Visualization Libraries\n",
        "# ============================================\n",
        "import seaborn as sns  # For statistical data visualization\n",
        "import matplotlib.pyplot as plt  # For general plotting\n",
        "\n",
        "# ============================================\n",
        "# 6. Model Persistence\n",
        "# ============================================\n",
        "import joblib  # To save and load trained models\n",
        "\n",
        "# ============================================\n",
        "# 7. Load Training and Test Data from Kaggle Input Path\n",
        "# ============================================\n",
        "train_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 8. Handle Missing Values\n",
        "# ============================================\n",
        "def handle_missing_values(train_df, test_df, strategy=\"mean\"):\n",
        "    \"\"\"\n",
        "    Handles missing values in train and test DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "        train_df (pd.DataFrame): Training DataFrame.\n",
        "        test_df (pd.DataFrame): Testing DataFrame.\n",
        "        strategy (str): Strategy to handle missing values (\"mean\", \"median\", or \"most_frequent\").\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: Cleaned training and testing DataFrames.\n",
        "    \"\"\"\n",
        "    # Ensure both DataFrames have the same columns\n",
        "    common_columns = set(train_df.columns).intersection(set(test_df.columns))\n",
        "    train_df = train_df[list(common_columns)].copy()\n",
        "    test_df = test_df[list(common_columns)].copy()\n",
        "\n",
        "    # Separate numeric and categorical columns\n",
        "    num_cols = train_df.select_dtypes(include=[\"number\"]).columns\n",
        "    cat_cols = train_df.select_dtypes(exclude=[\"number\"]).columns\n",
        "\n",
        "    # Impute missing values for numeric columns\n",
        "    num_imputer = SimpleImputer(strategy=strategy)\n",
        "    train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
        "    test_df[num_cols] = num_imputer.transform(test_df[num_cols])\n",
        "\n",
        "    # Impute missing values for categorical columns\n",
        "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "    train_df[cat_cols] = cat_imputer.fit_transform(train_df[cat_cols])\n",
        "    test_df[cat_cols] = cat_imputer.transform(test_df[cat_cols])\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# ============================================\n",
        "# 9. Clean Data\n",
        "# ============================================\n",
        "try:\n",
        "    # Check columns before cleaning\n",
        "    print(\"Train DataFrame columns:\", train_df.columns.tolist())\n",
        "    print(\"Test DataFrame columns:\", test_df.columns.tolist())\n",
        "\n",
        "    # Clean missing values\n",
        "    train_df_cleaned, test_df_cleaned = handle_missing_values(train_df, test_df, strategy=\"mean\")\n",
        "\n",
        "    # Add the 'sii' column back to `train_df_cleaned`\n",
        "    if 'sii' in train_df.columns:\n",
        "        train_df_cleaned['sii'] = train_df['sii']\n",
        "\n",
        "    # Remove rows with null values in the 'sii' column\n",
        "    train_df_cleaned = train_df_cleaned.dropna(subset=['sii'])\n",
        "\n",
        "    # Rename train_df_cleaned back to train_df\n",
        "    train_df = train_df_cleaned\n",
        "    test_df = test_df_cleaned\n",
        "\n",
        "    # Display cleaned data\n",
        "    print(\"\\nCleaned Train DataFrame (renamed to train_df):\")\n",
        "    print(train_df.info())\n",
        "    print(\"\\nCleaned Test DataFrame:\")\n",
        "    print(test_df_cleaned.info())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "# ============================================\n",
        "# 10. Check for Remaining Null Values\n",
        "# ============================================\n",
        "print(\"\\nNull values in 'sii' column of Train DataFrame:\")\n",
        "print(train_df['sii'].isnull().sum())\n",
        "\n",
        "# ============================================\n",
        "# 11. Separate Features and Target Variable\n",
        "# ============================================\n",
        "X_train = train_df.drop(columns=['sii'])  # 'sii' is the target variable\n",
        "y_train = train_df['sii']\n",
        "\n",
        "# ============================================\n",
        "# 12. Preprocess Test Data\n",
        "# ============================================\n",
        "X_test = test_df.drop(columns=['sii'], errors='ignore')  # Remove 'sii' if it exists in test data\n",
        "\n",
        "# One-hot encode categorical features in both train_df and test_df\n",
        "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Ensure that train and test data have the same columns after one-hot encoding\n",
        "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# ============================================\n",
        "# 13. Encode Labels for Target Variable\n",
        "# ============================================\n",
        "if y_train.dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    y_train = le.fit_transform(y_train)\n",
        "elif y_train.dtype == 'float' or not np.issubdtype(y_train.dtype, np.integer):\n",
        "    y_train = y_train.astype(int)\n",
        "\n",
        "# ============================================\n",
        "# 14. Scale Features for Logistic Regression\n",
        "# ============================================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)  # Scale test data using the same scaler\n",
        "\n",
        "# ============================================\n",
        "# 15. Split Data into Training and Validation Sets\n",
        "# ============================================\n",
        "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 16. Initialize Models\n",
        "# ============================================\n",
        "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# ============================================\n",
        "# 17. Define and Train Voting Classifier\n",
        "# ============================================\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[  # List of individual models\n",
        "        ('logreg', logreg_model),\n",
        "        ('rf', rf_model),\n",
        "        ('adaboost', adaboost_model),\n",
        "        ('gb', gb_model),\n",
        "        ('dt', dt_model),\n",
        "        ('xgb', xgb_model)\n",
        "    ],\n",
        "    voting='hard'  # Use hard voting for classification\n",
        ")\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf.fit(X_train_split, y_train_split)\n",
        "\n",
        "# ============================================\n",
        "# 18. Evaluate the Voting Classifier\n",
        "# ============================================\n",
        "y_valid_pred = voting_clf.predict(X_valid_split)\n",
        "validation_accuracy = accuracy_score(y_valid_split, y_valid_pred)\n",
        "print(f\"Validation Accuracy (Voting Classifier): {validation_accuracy:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# 19. Make Predictions on Test Set\n",
        "# ============================================\n",
        "test_predictions = voting_clf.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "0Nmc82o3ma0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}